{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyPi+/A6516HYJt9AAME2O1S",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU",
    "gpuClass": "standard"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/mostafa-ja/Data-Privacy/blob/main/Preserving_Data_Privacy_in_Deep_Learning2.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 115,
      "metadata": {
        "id": "U42vj0iALoZ_"
      },
      "outputs": [],
      "source": [
        "import os\n",
        "import random\n",
        "from tqdm import tqdm\n",
        "import numpy as np\n",
        "import torch, torchvision\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "import torch.optim as optim\n",
        "from torchvision import datasets, transforms\n",
        "from torch.utils.data.dataset import Dataset \n",
        "torch.backends.cudnn.benchmark=True"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "##### Hyperparameters for federated learning #########\n",
        "num_clients = 20\n",
        "num_selected = 12\n",
        "num_rounds = 200\n",
        "num_samples = 3\n",
        "batch_size = 32"
      ],
      "metadata": {
        "id": "PurEzUrS_vSM"
      },
      "execution_count": 116,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**3. Loading and dividing CIFAR 10 into clients**\n",
        "\n",
        "CIFAR10 dataset is used in this tutorial. It consists of 60,000 color images of 32x32 pixels in 10 classes. There are 50,000 training images and 10,000 test images. In the training batch, there are 5,000 images from each class, which makes 50,000 in total.\n",
        "\n",
        "In this tutorial, images are equally divided into clients, thus representing the balanced (IID) case."
      ],
      "metadata": {
        "id": "iFRgOpbjImjB"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "\n",
        "```\n",
        " generator = torch.Generator().manual_seed(42)\n",
        " random_split(range(10), [3, 7], generator=generator)\n",
        "\n",
        " train_data.data.shape[0]\n",
        "```\n",
        "\n"
      ],
      "metadata": {
        "id": "rG3I695vEFxH"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "#############################################################\n",
        "##### Creating desired data distribution among clients  #####\n",
        "#############################################################\n",
        "\n",
        "# Image augmentation \n",
        "transform_train = transforms.Compose([\n",
        "    transforms.RandomCrop(32,padding=4),\n",
        "    transforms.RandomHorizontalFlip(),\n",
        "    transforms.ToTensor(),\n",
        "    transforms.Normalize((0.4914, 0.4822, 0.4465), (0.2023, 0.1994, 0.2010))\n",
        "])\n",
        "\n",
        "# Loading CIFAR10 using torchvision.datasets\n",
        "train_data = datasets.CIFAR10('./train_data',train=True,download=True,\n",
        "                              transform=transform_train)\n",
        "\n",
        "# Dividing the training data into num_clients, with each client having equal number of images\n",
        "train_data_split = torch.utils.data.random_split(train_data,[int(train_data.data.shape[0]/num_clients) for _ in range(num_clients) ])\n",
        "\n",
        "# Creating a pytorch loader for a Deep Learning model\n",
        "train_loader = [torch.utils.data.DataLoader(x,batch_size=batch_size,shuffle=True) for x in train_data_split ]\n",
        "\n",
        "# Normalizing the test images\n",
        "transform_test = transforms.Compose([\n",
        "    transforms.ToTensor(),\n",
        "    transforms.Normalize((0.4914, 0.4822, 0.4465), (0.2023, 0.1994, 0.2010)),\n",
        "])\n",
        "\n",
        "# Loading the test iamges and thus converting them into a test_loader\n",
        "test_data = datasets.CIFAR10('./test_data',train=False,download=True,transform=transform_test)\n",
        "test_loader = torch.utils.data.DataLoader(test_data,batch_size=batch_size,shuffle=True)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "RJAxl19cBl5c",
        "outputId": "4523c13c-4d4c-4dfa-a60f-a881731732e1"
      },
      "execution_count": 117,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Files already downloaded and verified\n",
            "Files already downloaded and verified\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**4. Building the Neural Network (Model Architecture)**\n",
        "\n",
        "VGG19 (16 convolution layers, 3 Fully Connected layers, 5 MaxPool layers, and 1 SoftMax layer) are used in this tutorial. There are other variants of VGG like VGG11, VGG13, and VGG16."
      ],
      "metadata": {
        "id": "2Y_STXb-IriA"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "#################################\n",
        "##### Neural Network model #####\n",
        "#################################\n",
        "\n",
        "cfg = {\n",
        "    'VGG11': [64, 'M', 128, 'M', 256, 256, 'M', 512, 512, 'M', 512, 512, 'M'],\n",
        "    'VGG13': [64, 64, 'M', 128, 128, 'M', 256, 256, 'M', 512, 512, 'M', 512, 512, 'M'],\n",
        "    'VGG16': [64, 64, 'M', 128, 128, 'M', 256, 256, 256, 'M', 512, 512, 512, 'M', 512, 512, 512, 'M'],\n",
        "    'VGG19': [64, 64, 'M', 128, 128, 'M', 256, 256, 256, 256, 'M', 512, 512, 512, 512, 'M', 512, 512, 512, 512, 'M'],\n",
        "}\n",
        "\n",
        "class VGG(nn.Module):\n",
        "  def __init__(self,vgg_name):\n",
        "    super(VGG,self).__init__()\n",
        "    self.features = self.make_layers(cfg[vgg_name])\n",
        "    self.classifier = nn.Sequential(\n",
        "        nn.Linear(512,512),\n",
        "        nn.ReLU(True),\n",
        "        nn.Linear(512,512),\n",
        "        nn.ReLU(True),\n",
        "        nn.Linear(512,10)\n",
        "    )\n",
        "\n",
        "  def forward(self,x):\n",
        "    out = self.features(x)\n",
        "    out = out.view(out.size(0),-1)\n",
        "    out = self.classifier(out)\n",
        "    output = F.log_softmax(out,dim=1)\n",
        "    return output\n",
        "\n",
        "  def make_layers(self,cfg):\n",
        "    layers = []\n",
        "    in_channels = 3\n",
        "    for x in cfg:\n",
        "      if x == 'M':\n",
        "        layers += [nn.MaxPool2d(kernel_size=2,stride=2)]\n",
        "      else:\n",
        "        layers += [nn.Conv2d(in_channels,x,kernel_size=3,padding=1),\n",
        "                   nn.BatchNorm2d(x),\n",
        "                   nn.ReLU(inplace=True)]\n",
        "        in_channels = x\n",
        "    layers += [nn.AvgPool2d(kernel_size=1,stride=1)]\n",
        "    return nn.Sequential(*layers)\n"
      ],
      "metadata": {
        "id": "vXTtz06RIX5f"
      },
      "execution_count": 118,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**5. Helper functions for Federated training**\n",
        "\n",
        "The client_update function train the client model on private client data. This is the local training round that takes place at num_selected clients, i.e. 6 in our case."
      ],
      "metadata": {
        "id": "DSDDdbkEPH4s"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")"
      ],
      "metadata": {
        "id": "0YbeZrkKRrlw"
      },
      "execution_count": 119,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "data,target = next(iter(train_loader[2]))\n",
        "print(target)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "lc3FL7U80t8z",
        "outputId": "53af254d-a3b5-43a0-8d66-f9cfbed8abf3"
      },
      "execution_count": 120,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "tensor([1, 7, 1, 0, 5, 8, 9, 0, 2, 3, 7, 1, 9, 2, 2, 8, 9, 4, 7, 5, 5, 8, 2, 5,\n",
            "        8, 8, 5, 5, 1, 3, 5, 3])\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "50,000 training images\n",
        "\n",
        "each client has 2500 images\n",
        "```\n",
        "num_clients = 20\n",
        "num_selected = 12\n",
        "num_rounds = 200\n",
        "num_samples = 3\n",
        "batch_size = 32\n",
        "```\n",
        "every update , 3*32 data (3 samples ,batch_size=32) we use for training, so after about 25 rounds (2500/(3*32)) we have one epoch , just consider that we dont use all data , beacuse we access just 12 out of 20 clients, so every 25 rounds is about one epoch for accessed data"
      ],
      "metadata": {
        "id": "_pEonE7m5A25"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def client_update(client_model,optimizer,train_loader,epoch=5):\n",
        "    \"\"\"\n",
        "    This function updates/trains client model on client data\n",
        "    \"\"\"\n",
        "    client_model.train()\n",
        "    for s in range(num_samples):\n",
        "      data,target = next(iter(train_loader))\n",
        "      data,target = data.to(device), target.to(device)\n",
        "      optimizer.zero_grad()\n",
        "      output = client_model(data)\n",
        "      loss = F.nll_loss(output,target)\n",
        "      loss.backward()\n",
        "      optimizer.step()\n",
        "    return loss.item()"
      ],
      "metadata": {
        "id": "vCmrxZWnz-zR"
      },
      "execution_count": 121,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "The server_aggregate function aggregates the model weights received from every client and updates the global model with the updated weights. In this tutorial, the mean of the weights is taken and aggregated into the global weights."
      ],
      "metadata": {
        "id": "kXDB4nKiSKRF"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "global_model =  VGG('VGG19')\n",
        "global_model.state_dict().keys()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "sTUHJTymc4HI",
        "outputId": "a56937f9-b192-4b4d-9451-6cfb6b6be650"
      },
      "execution_count": 122,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "odict_keys(['features.0.weight', 'features.0.bias', 'features.1.weight', 'features.1.bias', 'features.1.running_mean', 'features.1.running_var', 'features.1.num_batches_tracked', 'features.3.weight', 'features.3.bias', 'features.4.weight', 'features.4.bias', 'features.4.running_mean', 'features.4.running_var', 'features.4.num_batches_tracked', 'features.7.weight', 'features.7.bias', 'features.8.weight', 'features.8.bias', 'features.8.running_mean', 'features.8.running_var', 'features.8.num_batches_tracked', 'features.10.weight', 'features.10.bias', 'features.11.weight', 'features.11.bias', 'features.11.running_mean', 'features.11.running_var', 'features.11.num_batches_tracked', 'features.14.weight', 'features.14.bias', 'features.15.weight', 'features.15.bias', 'features.15.running_mean', 'features.15.running_var', 'features.15.num_batches_tracked', 'features.17.weight', 'features.17.bias', 'features.18.weight', 'features.18.bias', 'features.18.running_mean', 'features.18.running_var', 'features.18.num_batches_tracked', 'features.20.weight', 'features.20.bias', 'features.21.weight', 'features.21.bias', 'features.21.running_mean', 'features.21.running_var', 'features.21.num_batches_tracked', 'features.23.weight', 'features.23.bias', 'features.24.weight', 'features.24.bias', 'features.24.running_mean', 'features.24.running_var', 'features.24.num_batches_tracked', 'features.27.weight', 'features.27.bias', 'features.28.weight', 'features.28.bias', 'features.28.running_mean', 'features.28.running_var', 'features.28.num_batches_tracked', 'features.30.weight', 'features.30.bias', 'features.31.weight', 'features.31.bias', 'features.31.running_mean', 'features.31.running_var', 'features.31.num_batches_tracked', 'features.33.weight', 'features.33.bias', 'features.34.weight', 'features.34.bias', 'features.34.running_mean', 'features.34.running_var', 'features.34.num_batches_tracked', 'features.36.weight', 'features.36.bias', 'features.37.weight', 'features.37.bias', 'features.37.running_mean', 'features.37.running_var', 'features.37.num_batches_tracked', 'features.40.weight', 'features.40.bias', 'features.41.weight', 'features.41.bias', 'features.41.running_mean', 'features.41.running_var', 'features.41.num_batches_tracked', 'features.43.weight', 'features.43.bias', 'features.44.weight', 'features.44.bias', 'features.44.running_mean', 'features.44.running_var', 'features.44.num_batches_tracked', 'features.46.weight', 'features.46.bias', 'features.47.weight', 'features.47.bias', 'features.47.running_mean', 'features.47.running_var', 'features.47.num_batches_tracked', 'features.49.weight', 'features.49.bias', 'features.50.weight', 'features.50.bias', 'features.50.running_mean', 'features.50.running_var', 'features.50.num_batches_tracked', 'classifier.0.weight', 'classifier.0.bias', 'classifier.2.weight', 'classifier.2.bias', 'classifier.4.weight', 'classifier.4.bias'])"
            ]
          },
          "metadata": {},
          "execution_count": 122
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "global_model.state_dict()['features.0.weight']"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "uPX_RYRIe56m",
        "outputId": "af10134f-d9b1-4daf-8d8c-b6b65f7d9ca1"
      },
      "execution_count": 123,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "tensor([[[[ 0.1430, -0.0403, -0.0992],\n",
              "          [ 0.0137, -0.0334,  0.1496],\n",
              "          [ 0.0715,  0.0444,  0.0633]],\n",
              "\n",
              "         [[ 0.1898,  0.1119,  0.1620],\n",
              "          [ 0.0805,  0.0311, -0.0513],\n",
              "          [ 0.0409, -0.1489,  0.1449]],\n",
              "\n",
              "         [[-0.0779, -0.0619,  0.0919],\n",
              "          [-0.0861,  0.1827,  0.0919],\n",
              "          [-0.0201, -0.0285,  0.0824]]],\n",
              "\n",
              "\n",
              "        [[[ 0.0686, -0.1898, -0.0114],\n",
              "          [ 0.0943,  0.1052,  0.0698],\n",
              "          [-0.0813, -0.0034,  0.1622]],\n",
              "\n",
              "         [[-0.0069,  0.0103,  0.0844],\n",
              "          [ 0.0920, -0.1111,  0.1512],\n",
              "          [ 0.0348,  0.1457,  0.1795]],\n",
              "\n",
              "         [[ 0.0803,  0.0080,  0.1231],\n",
              "          [ 0.0350, -0.0426, -0.1458],\n",
              "          [ 0.1622,  0.0887,  0.1495]]],\n",
              "\n",
              "\n",
              "        [[[ 0.0567, -0.0378,  0.0183],\n",
              "          [ 0.1905, -0.1672,  0.1884],\n",
              "          [ 0.0363, -0.0400,  0.1214]],\n",
              "\n",
              "         [[-0.1369,  0.1198,  0.0906],\n",
              "          [-0.0950,  0.1273, -0.1112],\n",
              "          [ 0.1137, -0.0226,  0.1269]],\n",
              "\n",
              "         [[-0.1528,  0.0923, -0.1496],\n",
              "          [ 0.1748,  0.0369,  0.0776],\n",
              "          [-0.0208,  0.0405, -0.0100]]],\n",
              "\n",
              "\n",
              "        ...,\n",
              "\n",
              "\n",
              "        [[[ 0.1080, -0.1210, -0.0373],\n",
              "          [ 0.1212,  0.1292,  0.0585],\n",
              "          [-0.0108, -0.1038,  0.1600]],\n",
              "\n",
              "         [[ 0.0012,  0.1138, -0.0087],\n",
              "          [-0.0189, -0.0714, -0.0258],\n",
              "          [ 0.1079,  0.0767,  0.0816]],\n",
              "\n",
              "         [[ 0.1685,  0.0248, -0.1349],\n",
              "          [ 0.1513, -0.0404, -0.0514],\n",
              "          [-0.0894,  0.1265, -0.1471]]],\n",
              "\n",
              "\n",
              "        [[[-0.1637,  0.0863,  0.0274],\n",
              "          [ 0.1334, -0.0226,  0.0826],\n",
              "          [ 0.0341,  0.0365, -0.0149]],\n",
              "\n",
              "         [[-0.0176,  0.1890,  0.0854],\n",
              "          [ 0.0060, -0.1416,  0.1695],\n",
              "          [ 0.1257, -0.1691,  0.1720]],\n",
              "\n",
              "         [[ 0.0460,  0.1211, -0.0800],\n",
              "          [ 0.1919, -0.0076,  0.0217],\n",
              "          [ 0.0044, -0.0767,  0.1127]]],\n",
              "\n",
              "\n",
              "        [[[-0.0177,  0.1914,  0.0379],\n",
              "          [ 0.0718,  0.1652,  0.1764],\n",
              "          [-0.1712, -0.0829, -0.0775]],\n",
              "\n",
              "         [[-0.0472,  0.0362,  0.0853],\n",
              "          [ 0.1656,  0.1050,  0.0850],\n",
              "          [ 0.0795,  0.1889, -0.0637]],\n",
              "\n",
              "         [[-0.0463,  0.0302,  0.1273],\n",
              "          [-0.0023, -0.0464, -0.0590],\n",
              "          [-0.0169, -0.1751, -0.0830]]]])"
            ]
          },
          "metadata": {},
          "execution_count": 123
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def server_aggregate(global_model, client_models):\n",
        "  \"\"\"\n",
        "  This function has aggregation method 'mean'\n",
        "  \"\"\"\n",
        "  ### This will take simple mean of the weights of models ###\n",
        "  global_dict = global_model.state_dict()\n",
        "  for k in global_dict.keys():\n",
        "    global_dict[k] = torch.stack([client_models[i].state_dict()[k].float() for i in range(len(client_models))],0).mean(0)\n",
        "  \n",
        "  # update the server model and clients model\n",
        "  global_model.load_state_dict(global_dict)\n",
        "  for model in client_models:\n",
        "    model.load_state_dict(global_model.state_dict())\n"
      ],
      "metadata": {
        "id": "MSX5_fddSLPR"
      },
      "execution_count": 124,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "The test function is the standard function, which takes the global model along with the test loader as the input and returns the test loss and accuracy."
      ],
      "metadata": {
        "id": "lcZ74HYrgtM-"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def test(global_model,test_loader):\n",
        "  \"\"\"This function test the global model on test data and returns test loss and test accuracy \"\"\"\n",
        "  \n",
        "  global_model.eval()\n",
        "  test_loss = 0\n",
        "  correct = 0\n",
        "  with torch.no_grad():\n",
        "      for data, target in test_loader:\n",
        "          data, target = data.to(device), target.to(device)\n",
        "          output = global_model(data)\n",
        "          test_loss += F.nll_loss(output, target, reduction='sum').item()  # sum up batch loss\n",
        "          pred = output.argmax(dim=1, keepdim=True)  # get the index of the max log-probability\n",
        "          correct += pred.eq(target.view_as(pred)).sum().item()\n",
        "\n",
        "  test_loss /= len(test_loader.dataset)\n",
        "  acc = correct / len(test_loader.dataset)\n",
        "\n",
        "  return test_loss, acc\n"
      ],
      "metadata": {
        "id": "RahDvBgOgqn9"
      },
      "execution_count": 125,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**6. Training the model**\n",
        "\n",
        "One global model, along with the individual client_models is initialized with VGG19 on a GPU. In this tutorial, SGD is used as an optimizer for all the client models."
      ],
      "metadata": {
        "id": "9Gdpd9pUi3nV"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "############################################\n",
        "#### Initializing models and optimizer  ####\n",
        "############################################\n",
        "\n",
        "#### global model ##########\n",
        "global_model =  VGG('VGG19').to(device)\n",
        "\n",
        "############## client models ##############\n",
        "client_models = [ VGG('VGG19').to(device) for _ in range(num_selected)]\n",
        "for model in client_models:\n",
        "    model.load_state_dict(global_model.state_dict()) ### initial synchronizing with global model \n",
        "\n",
        "############### optimizers ################\n",
        "opt = [optim.SGD(model.parameters(), lr=0.1) for model in client_models]"
      ],
      "metadata": {
        "id": "b8EZXgRNi1Eu"
      },
      "execution_count": 126,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Instead of VGG19, one can also use VGG11, VGG13, and VGG16. Other optimizers are also available and one can check the link for more details."
      ],
      "metadata": {
        "id": "f8LrUIYWj7gO"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "np.random.permutation(num_clients)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "C3xF0NoGk9vE",
        "outputId": "40beeb9a-74ba-4780-a4cb-5ab1e02ae783"
      },
      "execution_count": 127,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array([ 4, 15, 16,  1,  8,  0,  9, 18, 12, 19, 11,  2,  6,  7,  5, 17, 10,\n",
              "        3, 14, 13])"
            ]
          },
          "metadata": {},
          "execution_count": 127
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "for i in tqdm(range(3)):\n",
        "  print(i)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "MFpD8MWdkgmY",
        "outputId": "72a58673-668b-46a9-d53d-138227b6d747"
      },
      "execution_count": 128,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 3/3 [00:00<00:00, 18808.54it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "0\n",
            "1\n",
            "2\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "50%26"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "pQ_SvCoD7-Zi",
        "outputId": "84379814-a249-45c1-acb0-79ca1e513304"
      },
      "execution_count": 129,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "24"
            ]
          },
          "metadata": {},
          "execution_count": 129
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "###### List containing info about learning #########\n",
        "losses_train = []\n",
        "losses_test = []\n",
        "acc_train = []\n",
        "acc_test = []\n",
        "# Runnining FL\n",
        "\n",
        "for r in range(num_rounds):\n",
        "    # select random clients\n",
        "    client_idx = np.random.permutation(num_clients)[:num_selected]\n",
        "    # client update\n",
        "    loss = 0\n",
        "    for i in tqdm(range(num_selected)):\n",
        "        loss += client_update(client_models[i], opt[i], train_loader[client_idx[i]], epoch=epochs)\n",
        "    \n",
        "    losses_train.append(loss)\n",
        "    # server aggregate\n",
        "    server_aggregate(global_model, client_models)\n",
        "    \n",
        "    test_loss, acc = test(global_model, test_loader)\n",
        "    losses_test.append(test_loss)\n",
        "    acc_test.append(acc)\n",
        "    if (r % 10 == 0) or (r == (num_rounds-1)):\n",
        "      print('%d-th round' % r)\n",
        "      print('average train loss %0.3g | test loss %0.3g | test acc: %0.3f' % (loss / num_selected, test_loss, acc))\n",
        "    "
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ATW2NQVwjbAb",
        "outputId": "202e5f86-cf5f-49ce-e6f9-4349ec24f60e"
      },
      "execution_count": 130,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 12/12 [00:01<00:00, 10.67it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "0-th round\n",
            "average train loss 2.33 | test loss 2.3 | test acc: 0.100\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 12/12 [00:01<00:00, 11.58it/s]\n",
            "100%|██████████| 12/12 [00:01<00:00, 11.67it/s]\n",
            "100%|██████████| 12/12 [00:01<00:00, 11.57it/s]\n",
            "100%|██████████| 12/12 [00:01<00:00, 11.60it/s]\n",
            "100%|██████████| 12/12 [00:01<00:00, 11.55it/s]\n",
            "100%|██████████| 12/12 [00:01<00:00, 11.43it/s]\n",
            "100%|██████████| 12/12 [00:01<00:00, 11.51it/s]\n",
            "100%|██████████| 12/12 [00:01<00:00,  9.92it/s]\n",
            "100%|██████████| 12/12 [00:01<00:00, 11.55it/s]\n",
            "100%|██████████| 12/12 [00:01<00:00,  8.92it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "10-th round\n",
            "average train loss 2.13 | test loss 2.14 | test acc: 0.184\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 12/12 [00:01<00:00, 11.65it/s]\n",
            "100%|██████████| 12/12 [00:01<00:00, 10.37it/s]\n",
            "100%|██████████| 12/12 [00:01<00:00, 11.66it/s]\n",
            "100%|██████████| 12/12 [00:01<00:00, 11.64it/s]\n",
            "100%|██████████| 12/12 [00:01<00:00,  9.47it/s]\n",
            "100%|██████████| 12/12 [00:01<00:00, 11.34it/s]\n",
            "100%|██████████| 12/12 [00:01<00:00, 11.42it/s]\n",
            "100%|██████████| 12/12 [00:01<00:00, 11.42it/s]\n",
            "100%|██████████| 12/12 [00:01<00:00, 11.46it/s]\n",
            "100%|██████████| 12/12 [00:01<00:00, 11.72it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "20-th round\n",
            "average train loss 2.03 | test loss 1.9 | test acc: 0.272\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 12/12 [00:01<00:00,  9.06it/s]\n",
            "100%|██████████| 12/12 [00:01<00:00, 11.47it/s]\n",
            "100%|██████████| 12/12 [00:01<00:00,  9.09it/s]\n",
            "100%|██████████| 12/12 [00:01<00:00, 11.61it/s]\n",
            "100%|██████████| 12/12 [00:01<00:00, 11.11it/s]\n",
            "100%|██████████| 12/12 [00:01<00:00, 11.60it/s]\n",
            "100%|██████████| 12/12 [00:01<00:00, 11.49it/s]\n",
            "100%|██████████| 12/12 [00:01<00:00, 11.67it/s]\n",
            "100%|██████████| 12/12 [00:01<00:00, 11.42it/s]\n",
            "100%|██████████| 12/12 [00:01<00:00, 11.62it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "30-th round\n",
            "average train loss 2.01 | test loss 1.64 | test acc: 0.384\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 12/12 [00:01<00:00, 11.54it/s]\n",
            "100%|██████████| 12/12 [00:01<00:00, 11.59it/s]\n",
            "100%|██████████| 12/12 [00:01<00:00, 11.65it/s]\n",
            "100%|██████████| 12/12 [00:01<00:00, 11.56it/s]\n",
            "100%|██████████| 12/12 [00:01<00:00, 11.42it/s]\n",
            "100%|██████████| 12/12 [00:01<00:00,  9.32it/s]\n",
            "100%|██████████| 12/12 [00:01<00:00, 11.45it/s]\n",
            "100%|██████████| 12/12 [00:01<00:00,  9.05it/s]\n",
            "100%|██████████| 12/12 [00:01<00:00, 11.55it/s]\n",
            "100%|██████████| 12/12 [00:01<00:00, 11.48it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "40-th round\n",
            "average train loss 2.04 | test loss 1.57 | test acc: 0.394\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 12/12 [00:01<00:00, 11.66it/s]\n",
            "100%|██████████| 12/12 [00:01<00:00, 11.54it/s]\n",
            "100%|██████████| 12/12 [00:01<00:00, 11.44it/s]\n",
            "100%|██████████| 12/12 [00:01<00:00, 11.54it/s]\n",
            "100%|██████████| 12/12 [00:01<00:00, 11.36it/s]\n",
            "100%|██████████| 12/12 [00:01<00:00, 11.56it/s]\n",
            "100%|██████████| 12/12 [00:01<00:00, 11.45it/s]\n",
            "100%|██████████| 12/12 [00:01<00:00, 11.72it/s]\n",
            "100%|██████████| 12/12 [00:01<00:00, 10.20it/s]\n",
            "100%|██████████| 12/12 [00:01<00:00, 11.71it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "50-th round\n",
            "average train loss 1.89 | test loss 1.57 | test acc: 0.407\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 12/12 [00:01<00:00,  8.94it/s]\n",
            "100%|██████████| 12/12 [00:01<00:00, 11.43it/s]\n",
            "100%|██████████| 12/12 [00:01<00:00,  9.77it/s]\n",
            "100%|██████████| 12/12 [00:01<00:00, 11.47it/s]\n",
            "100%|██████████| 12/12 [00:01<00:00, 11.53it/s]\n",
            "100%|██████████| 12/12 [00:01<00:00, 11.65it/s]\n",
            "100%|██████████| 12/12 [00:01<00:00, 11.44it/s]\n",
            "100%|██████████| 12/12 [00:01<00:00, 11.57it/s]\n",
            "100%|██████████| 12/12 [00:01<00:00, 11.53it/s]\n",
            "100%|██████████| 12/12 [00:01<00:00, 11.56it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "60-th round\n",
            "average train loss 1.76 | test loss 1.43 | test acc: 0.469\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 12/12 [00:01<00:00, 11.66it/s]\n",
            "100%|██████████| 12/12 [00:01<00:00, 11.60it/s]\n",
            "100%|██████████| 12/12 [00:01<00:00, 11.73it/s]\n",
            "100%|██████████| 12/12 [00:01<00:00, 11.60it/s]\n",
            "100%|██████████| 12/12 [00:01<00:00, 11.25it/s]\n",
            "100%|██████████| 12/12 [00:01<00:00,  9.41it/s]\n",
            "100%|██████████| 12/12 [00:01<00:00, 11.22it/s]\n",
            "100%|██████████| 12/12 [00:01<00:00,  8.82it/s]\n",
            "100%|██████████| 12/12 [00:01<00:00, 11.48it/s]\n",
            "100%|██████████| 12/12 [00:01<00:00, 10.47it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "70-th round\n",
            "average train loss 1.9 | test loss 1.37 | test acc: 0.506\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 12/12 [00:01<00:00, 11.57it/s]\n",
            "100%|██████████| 12/12 [00:01<00:00, 11.55it/s]\n",
            "100%|██████████| 12/12 [00:01<00:00, 10.65it/s]\n",
            "100%|██████████| 12/12 [00:01<00:00, 11.55it/s]\n",
            "100%|██████████| 12/12 [00:01<00:00, 11.53it/s]\n",
            "100%|██████████| 12/12 [00:01<00:00, 11.64it/s]\n",
            "100%|██████████| 12/12 [00:01<00:00, 11.21it/s]\n",
            "100%|██████████| 12/12 [00:01<00:00, 11.38it/s]\n",
            "100%|██████████| 12/12 [00:01<00:00, 11.27it/s]\n",
            "100%|██████████| 12/12 [00:01<00:00, 11.55it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "80-th round\n",
            "average train loss 1.66 | test loss 1.3 | test acc: 0.533\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 12/12 [00:01<00:00,  9.31it/s]\n",
            "100%|██████████| 12/12 [00:01<00:00, 11.61it/s]\n",
            "100%|██████████| 12/12 [00:01<00:00,  9.10it/s]\n",
            "100%|██████████| 12/12 [00:01<00:00, 11.24it/s]\n",
            "100%|██████████| 12/12 [00:01<00:00, 10.88it/s]\n",
            "100%|██████████| 12/12 [00:01<00:00, 11.45it/s]\n",
            "100%|██████████| 12/12 [00:01<00:00, 11.43it/s]\n",
            "100%|██████████| 12/12 [00:01<00:00, 11.50it/s]\n",
            "100%|██████████| 12/12 [00:01<00:00, 11.67it/s]\n",
            "100%|██████████| 12/12 [00:01<00:00, 11.48it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "90-th round\n",
            "average train loss 1.62 | test loss 1.31 | test acc: 0.517\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 12/12 [00:01<00:00, 11.46it/s]\n",
            "100%|██████████| 12/12 [00:01<00:00, 11.56it/s]\n",
            "100%|██████████| 12/12 [00:01<00:00, 11.37it/s]\n",
            "100%|██████████| 12/12 [00:01<00:00, 10.08it/s]\n",
            "100%|██████████| 12/12 [00:01<00:00, 11.31it/s]\n",
            "100%|██████████| 12/12 [00:01<00:00,  8.91it/s]\n",
            "100%|██████████| 12/12 [00:01<00:00, 11.60it/s]\n",
            "100%|██████████| 12/12 [00:01<00:00,  9.51it/s]\n",
            "100%|██████████| 12/12 [00:01<00:00, 11.64it/s]\n",
            "100%|██████████| 12/12 [00:01<00:00, 11.74it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "100-th round\n",
            "average train loss 1.61 | test loss 1.29 | test acc: 0.532\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 12/12 [00:01<00:00, 11.65it/s]\n",
            "100%|██████████| 12/12 [00:01<00:00, 11.48it/s]\n",
            "100%|██████████| 12/12 [00:01<00:00, 11.61it/s]\n",
            "100%|██████████| 12/12 [00:01<00:00, 11.70it/s]\n",
            "100%|██████████| 12/12 [00:01<00:00, 11.54it/s]\n",
            "100%|██████████| 12/12 [00:01<00:00, 11.69it/s]\n",
            "100%|██████████| 12/12 [00:01<00:00, 11.64it/s]\n",
            "100%|██████████| 12/12 [00:01<00:00, 11.53it/s]\n",
            "100%|██████████| 12/12 [00:01<00:00, 11.65it/s]\n",
            "100%|██████████| 12/12 [00:01<00:00, 11.60it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "110-th round\n",
            "average train loss 1.5 | test loss 1.15 | test acc: 0.584\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 12/12 [00:01<00:00,  9.24it/s]\n",
            "100%|██████████| 12/12 [00:01<00:00, 11.64it/s]\n",
            "100%|██████████| 12/12 [00:01<00:00, 10.00it/s]\n",
            "100%|██████████| 12/12 [00:01<00:00, 11.37it/s]\n",
            "100%|██████████| 12/12 [00:01<00:00, 11.62it/s]\n",
            "100%|██████████| 12/12 [00:01<00:00, 11.67it/s]\n",
            "100%|██████████| 12/12 [00:01<00:00, 11.14it/s]\n",
            "100%|██████████| 12/12 [00:01<00:00, 11.68it/s]\n",
            "100%|██████████| 12/12 [00:01<00:00, 11.41it/s]\n",
            "100%|██████████| 12/12 [00:01<00:00, 11.67it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "120-th round\n",
            "average train loss 1.46 | test loss 1.08 | test acc: 0.621\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 12/12 [00:01<00:00, 11.23it/s]\n",
            "100%|██████████| 12/12 [00:01<00:00, 11.57it/s]\n",
            "100%|██████████| 12/12 [00:01<00:00, 11.74it/s]\n",
            "100%|██████████| 12/12 [00:01<00:00, 11.57it/s]\n",
            "100%|██████████| 12/12 [00:01<00:00, 11.68it/s]\n",
            "100%|██████████| 12/12 [00:01<00:00,  9.77it/s]\n",
            "100%|██████████| 12/12 [00:01<00:00, 11.71it/s]\n",
            "100%|██████████| 12/12 [00:01<00:00,  9.01it/s]\n",
            "100%|██████████| 12/12 [00:01<00:00, 11.58it/s]\n",
            "100%|██████████| 12/12 [00:01<00:00,  9.56it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "130-th round\n",
            "average train loss 1.55 | test loss 1.13 | test acc: 0.596\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 12/12 [00:01<00:00, 11.86it/s]\n",
            "100%|██████████| 12/12 [00:01<00:00, 11.59it/s]\n",
            "100%|██████████| 12/12 [00:01<00:00,  9.46it/s]\n",
            "100%|██████████| 12/12 [00:01<00:00, 11.64it/s]\n",
            "100%|██████████| 12/12 [00:01<00:00, 11.62it/s]\n",
            "100%|██████████| 12/12 [00:01<00:00, 11.58it/s]\n",
            "100%|██████████| 12/12 [00:01<00:00, 11.71it/s]\n",
            "100%|██████████| 12/12 [00:01<00:00, 11.71it/s]\n",
            "100%|██████████| 12/12 [00:01<00:00, 11.71it/s]\n",
            "100%|██████████| 12/12 [00:01<00:00, 11.71it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "140-th round\n",
            "average train loss 1.44 | test loss 1.04 | test acc: 0.634\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 12/12 [00:01<00:00, 11.69it/s]\n",
            "100%|██████████| 12/12 [00:01<00:00, 11.75it/s]\n",
            "100%|██████████| 12/12 [00:01<00:00,  9.79it/s]\n",
            "100%|██████████| 12/12 [00:01<00:00, 11.74it/s]\n",
            "100%|██████████| 12/12 [00:01<00:00,  9.18it/s]\n",
            "100%|██████████| 12/12 [00:01<00:00, 11.52it/s]\n",
            "100%|██████████| 12/12 [00:01<00:00,  9.86it/s]\n",
            "100%|██████████| 12/12 [00:01<00:00, 11.66it/s]\n",
            "100%|██████████| 12/12 [00:01<00:00, 11.42it/s]\n",
            "100%|██████████| 12/12 [00:01<00:00, 11.65it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "150-th round\n",
            "average train loss 1.5 | test loss 1.02 | test acc: 0.639\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 12/12 [00:01<00:00, 11.39it/s]\n",
            "100%|██████████| 12/12 [00:01<00:00, 11.50it/s]\n",
            "100%|██████████| 12/12 [00:01<00:00, 11.69it/s]\n",
            "100%|██████████| 12/12 [00:01<00:00, 11.65it/s]\n",
            "100%|██████████| 12/12 [00:01<00:00, 11.51it/s]\n",
            "100%|██████████| 12/12 [00:01<00:00, 11.84it/s]\n",
            "100%|██████████| 12/12 [00:01<00:00, 11.62it/s]\n",
            "100%|██████████| 12/12 [00:01<00:00, 11.50it/s]\n",
            "100%|██████████| 12/12 [00:01<00:00, 11.50it/s]\n",
            "100%|██████████| 12/12 [00:01<00:00,  9.58it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "160-th round\n",
            "average train loss 1.45 | test loss 1.02 | test acc: 0.624\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 12/12 [00:01<00:00, 11.47it/s]\n",
            "100%|██████████| 12/12 [00:01<00:00,  9.05it/s]\n",
            "100%|██████████| 12/12 [00:01<00:00, 11.27it/s]\n",
            "100%|██████████| 12/12 [00:01<00:00, 10.06it/s]\n",
            "100%|██████████| 12/12 [00:01<00:00, 11.68it/s]\n",
            "100%|██████████| 12/12 [00:01<00:00, 11.66it/s]\n",
            "100%|██████████| 12/12 [00:01<00:00, 11.60it/s]\n",
            "100%|██████████| 12/12 [00:01<00:00, 11.73it/s]\n",
            "100%|██████████| 12/12 [00:01<00:00, 11.74it/s]\n",
            "100%|██████████| 12/12 [00:01<00:00, 11.61it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "170-th round\n",
            "average train loss 1.48 | test loss 1.06 | test acc: 0.624\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 12/12 [00:01<00:00, 11.68it/s]\n",
            "100%|██████████| 12/12 [00:01<00:00, 11.58it/s]\n",
            "100%|██████████| 12/12 [00:01<00:00, 11.70it/s]\n",
            "100%|██████████| 12/12 [00:01<00:00, 11.53it/s]\n",
            "100%|██████████| 12/12 [00:01<00:00, 11.80it/s]\n",
            "100%|██████████| 12/12 [00:01<00:00, 11.82it/s]\n",
            "100%|██████████| 12/12 [00:01<00:00, 11.45it/s]\n",
            "100%|██████████| 12/12 [00:01<00:00, 11.53it/s]\n",
            "100%|██████████| 12/12 [00:01<00:00,  9.36it/s]\n",
            "100%|██████████| 12/12 [00:01<00:00, 11.60it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "180-th round\n",
            "average train loss 1.44 | test loss 0.89 | test acc: 0.695\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 12/12 [00:01<00:00, 10.22it/s]\n",
            "100%|██████████| 12/12 [00:01<00:00, 11.52it/s]\n",
            "100%|██████████| 12/12 [00:01<00:00, 11.56it/s]\n",
            "100%|██████████| 12/12 [00:01<00:00, 11.58it/s]\n",
            "100%|██████████| 12/12 [00:01<00:00, 11.67it/s]\n",
            "100%|██████████| 12/12 [00:01<00:00, 11.64it/s]\n",
            "100%|██████████| 12/12 [00:01<00:00, 11.77it/s]\n",
            "100%|██████████| 12/12 [00:01<00:00, 11.59it/s]\n",
            "100%|██████████| 12/12 [00:01<00:00, 11.79it/s]\n",
            "100%|██████████| 12/12 [00:01<00:00, 11.61it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "190-th round\n",
            "average train loss 1.24 | test loss 0.888 | test acc: 0.695\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 12/12 [00:01<00:00, 11.62it/s]\n",
            "100%|██████████| 12/12 [00:01<00:00, 11.56it/s]\n",
            "100%|██████████| 12/12 [00:01<00:00, 11.63it/s]\n",
            "100%|██████████| 12/12 [00:01<00:00, 10.57it/s]\n",
            "100%|██████████| 12/12 [00:01<00:00, 11.60it/s]\n",
            "100%|██████████| 12/12 [00:01<00:00,  8.93it/s]\n",
            "100%|██████████| 12/12 [00:01<00:00, 11.60it/s]\n",
            "100%|██████████| 12/12 [00:01<00:00,  8.94it/s]\n",
            "100%|██████████| 12/12 [00:01<00:00, 11.52it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "199-th round\n",
            "average train loss 1.18 | test loss 0.823 | test acc: 0.717\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "epochs = 5\n",
        "batch_size = 32\n",
        "\n",
        "train_loader = torch.utils.data.DataLoader(train_data,batch_size=batch_size,shuffle=True)\n",
        "model =  VGG('VGG19')\n",
        "model.to(device)\n",
        "optimizer = optim.SGD(model.parameters(), lr=0.1)\n",
        "\n",
        "for e in range(epochs):\n",
        "  for batch_idx, (data,target) in enumerate(train_loader):\n",
        "    data,target = data.to(device), target.to(device)\n",
        "    optimizer.zero_grad()\n",
        "    output = model(data)\n",
        "    loss = F.nll_loss(output,target)\n",
        "    loss.backward()\n",
        "    optimizer.step()\n",
        "\n",
        "  model.eval()\n",
        "  test_loss = 0\n",
        "  correct = 0\n",
        "  with torch.no_grad():\n",
        "      for data, target in test_loader:\n",
        "          data, target = data.cuda(), target.cuda()\n",
        "          output = model(data)\n",
        "          test_loss += F.nll_loss(output, target, reduction='sum').item()  # sum up batch loss\n",
        "          pred = output.argmax(dim=1, keepdim=True)  # get the index of the max log-probability\n",
        "          correct += pred.eq(target.view_as(pred)).sum().item()\n",
        "  test_loss /= len(test_loader.dataset)\n",
        "  acc = correct / len(test_loader.dataset)\n",
        "  print('%d-th epoch' % e)\n",
        "  print('average train loss %0.3g | test loss %0.3g | test acc: %0.3f' % (loss.item(), test_loss, acc))\n",
        "  model.train()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "wAtqb98-rHr4",
        "outputId": "bcbf1040-4eaf-4297-8064-f8167b08f17b"
      },
      "execution_count": 131,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "0-th epoch\n",
            "average train loss 1.64 | test loss 1.7 | test acc: 0.426\n",
            "1-th epoch\n",
            "average train loss 1.11 | test loss 1.17 | test acc: 0.597\n",
            "2-th epoch\n",
            "average train loss 1.22 | test loss 0.931 | test acc: 0.678\n",
            "3-th epoch\n",
            "average train loss 0.356 | test loss 0.688 | test acc: 0.772\n",
            "4-th epoch\n",
            "average train loss 0.64 | test loss 0.702 | test acc: 0.766\n"
          ]
        }
      ]
    }
  ]
}