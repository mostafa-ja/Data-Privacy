{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyPi+/A6516HYJt9AAME2O1S",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU",
    "gpuClass": "standard"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/mostafa-ja/Data-Privacy/blob/main/Preserving_Data_Privacy_in_Deep_Learning2.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 115,
      "metadata": {
        "id": "U42vj0iALoZ_"
      },
      "outputs": [],
      "source": [
        "import os\n",
        "import random\n",
        "from tqdm import tqdm\n",
        "import numpy as np\n",
        "import torch, torchvision\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "import torch.optim as optim\n",
        "from torchvision import datasets, transforms\n",
        "from torch.utils.data.dataset import Dataset \n",
        "torch.backends.cudnn.benchmark=True"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "##### Hyperparameters for federated learning #########\n",
        "num_clients = 20\n",
        "num_selected = 12\n",
        "num_rounds = 200\n",
        "num_samples = 3\n",
        "batch_size = 32"
      ],
      "metadata": {
        "id": "PurEzUrS_vSM"
      },
      "execution_count": 116,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**3. Loading and dividing CIFAR 10 into clients**\n",
        "\n",
        "CIFAR10 dataset is used in this tutorial. It consists of 60,000 color images of 32x32 pixels in 10 classes. There are 50,000 training images and 10,000 test images. In the training batch, there are 5,000 images from each class, which makes 50,000 in total.\n",
        "\n",
        "In this tutorial, images are equally divided into clients, thus representing the balanced (IID) case."
      ],
      "metadata": {
        "id": "iFRgOpbjImjB"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "\n",
        "```\n",
        " generator = torch.Generator().manual_seed(42)\n",
        " random_split(range(10), [3, 7], generator=generator)\n",
        "\n",
        " train_data.data.shape[0]\n",
        "```\n",
        "\n"
      ],
      "metadata": {
        "id": "rG3I695vEFxH"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "#############################################################\n",
        "##### Creating desired data distribution among clients  #####\n",
        "#############################################################\n",
        "\n",
        "# Image augmentation \n",
        "transform_train = transforms.Compose([\n",
        "    transforms.RandomCrop(32,padding=4),\n",
        "    transforms.RandomHorizontalFlip(),\n",
        "    transforms.ToTensor(),\n",
        "    transforms.Normalize((0.4914, 0.4822, 0.4465), (0.2023, 0.1994, 0.2010))\n",
        "])\n",
        "\n",
        "# Loading CIFAR10 using torchvision.datasets\n",
        "train_data = datasets.CIFAR10('./train_data',train=True,download=True,\n",
        "                              transform=transform_train)\n",
        "\n",
        "# Dividing the training data into num_clients, with each client having equal number of images\n",
        "train_data_split = torch.utils.data.random_split(train_data,[int(train_data.data.shape[0]/num_clients) for _ in range(num_clients) ])\n",
        "\n",
        "# Creating a pytorch loader for a Deep Learning model\n",
        "train_loader = [torch.utils.data.DataLoader(x,batch_size=batch_size,shuffle=True) for x in train_data_split ]\n",
        "\n",
        "# Normalizing the test images\n",
        "transform_test = transforms.Compose([\n",
        "    transforms.ToTensor(),\n",
        "    transforms.Normalize((0.4914, 0.4822, 0.4465), (0.2023, 0.1994, 0.2010)),\n",
        "])\n",
        "\n",
        "# Loading the test iamges and thus converting them into a test_loader\n",
        "test_data = datasets.CIFAR10('./test_data',train=False,download=True,transform=transform_test)\n",
        "test_loader = torch.utils.data.DataLoader(test_data,batch_size=batch_size,shuffle=True)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "RJAxl19cBl5c",
        "outputId": "4523c13c-4d4c-4dfa-a60f-a881731732e1"
      },
      "execution_count": 117,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Files already downloaded and verified\n",
            "Files already downloaded and verified\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**4. Building the Neural Network (Model Architecture)**\n",
        "\n",
        "VGG19 (16 convolution layers, 3 Fully Connected layers, 5 MaxPool layers, and 1 SoftMax layer) are used in this tutorial. There are other variants of VGG like VGG11, VGG13, and VGG16."
      ],
      "metadata": {
        "id": "2Y_STXb-IriA"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "#################################\n",
        "##### Neural Network model #####\n",
        "#################################\n",
        "\n",
        "cfg = {\n",
        "    'VGG11': [64, 'M', 128, 'M', 256, 256, 'M', 512, 512, 'M', 512, 512, 'M'],\n",
        "    'VGG13': [64, 64, 'M', 128, 128, 'M', 256, 256, 'M', 512, 512, 'M', 512, 512, 'M'],\n",
        "    'VGG16': [64, 64, 'M', 128, 128, 'M', 256, 256, 256, 'M', 512, 512, 512, 'M', 512, 512, 512, 'M'],\n",
        "    'VGG19': [64, 64, 'M', 128, 128, 'M', 256, 256, 256, 256, 'M', 512, 512, 512, 512, 'M', 512, 512, 512, 512, 'M'],\n",
        "}\n",
        "\n",
        "class VGG(nn.Module):\n",
        "  def __init__(self,vgg_name):\n",
        "    super(VGG,self).__init__()\n",
        "    self.features = self.make_layers(cfg[vgg_name])\n",
        "    self.classifier = nn.Sequential(\n",
        "        nn.Linear(512,512),\n",
        "        nn.ReLU(True),\n",
        "        nn.Linear(512,512),\n",
        "        nn.ReLU(True),\n",
        "        nn.Linear(512,10)\n",
        "    )\n",
        "\n",
        "  def forward(self,x):\n",
        "    out = self.features(x)\n",
        "    out = out.view(out.size(0),-1)\n",
        "    out = self.classifier(out)\n",
        "    output = F.log_softmax(out,dim=1)\n",
        "    return output\n",
        "\n",
        "  def make_layers(self,cfg):\n",
        "    layers = []\n",
        "    in_channels = 3\n",
        "    for x in cfg:\n",
        "      if x == 'M':\n",
        "        layers += [nn.MaxPool2d(kernel_size=2,stride=2)]\n",
        "      else:\n",
        "        layers += [nn.Conv2d(in_channels,x,kernel_size=3,padding=1),\n",
        "                   nn.BatchNorm2d(x),\n",
        "                   nn.ReLU(inplace=True)]\n",
        "        in_channels = x\n",
        "    layers += [nn.AvgPool2d(kernel_size=1,stride=1)]\n",
        "    return nn.Sequential(*layers)\n"
      ],
      "metadata": {
        "id": "vXTtz06RIX5f"
      },
      "execution_count": 118,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**5. Helper functions for Federated training**\n",
        "\n",
        "The client_update function train the client model on private client data. This is the local training round that takes place at num_selected clients, i.e. 6 in our case."
      ],
      "metadata": {
        "id": "DSDDdbkEPH4s"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")"
      ],
      "metadata": {
        "id": "0YbeZrkKRrlw"
      },
      "execution_count": 119,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "data,target = next(iter(train_loader[2]))\n",
        "print(target)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "lc3FL7U80t8z",
        "outputId": "53af254d-a3b5-43a0-8d66-f9cfbed8abf3"
      },
      "execution_count": 120,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "tensor([1, 7, 1, 0, 5, 8, 9, 0, 2, 3, 7, 1, 9, 2, 2, 8, 9, 4, 7, 5, 5, 8, 2, 5,\n",
            "        8, 8, 5, 5, 1, 3, 5, 3])\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "50,000 training images\n",
        "\n",
        "each client has 2500 images\n",
        "```\n",
        "num_clients = 20\n",
        "num_selected = 12\n",
        "num_rounds = 200\n",
        "num_samples = 3\n",
        "batch_size = 32\n",
        "```\n",
        "every update , 3*32 data (3 samples ,batch_size=32) we use for training, so after about 25 rounds (2500/(3*32)) we have one epoch , just consider that we dont use all data , beacuse we access just 12 out of 20 clients, so every 25 rounds is about one epoch for accessed data"
      ],
      "metadata": {
        "id": "_pEonE7m5A25"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def client_update(client_model,optimizer,train_loader,epoch=5):\n",
        "    \"\"\"\n",
        "    This function updates/trains client model on client data\n",
        "    \"\"\"\n",
        "    client_model.train()\n",
        "    for s in range(num_samples):\n",
        "      data,target = next(iter(train_loader))\n",
        "      data,target = data.to(device), target.to(device)\n",
        "      optimizer.zero_grad()\n",
        "      output = client_model(data)\n",
        "      loss = F.nll_loss(output,target)\n",
        "      loss.backward()\n",
        "      optimizer.step()\n",
        "    return loss.item()"
      ],
      "metadata": {
        "id": "vCmrxZWnz-zR"
      },
      "execution_count": 121,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "The server_aggregate function aggregates the model weights received from every client and updates the global model with the updated weights. In this tutorial, the mean of the weights is taken and aggregated into the global weights."
      ],
      "metadata": {
        "id": "kXDB4nKiSKRF"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "global_model =  VGG('VGG19')\n",
        "global_model.state_dict().keys()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "sTUHJTymc4HI",
        "outputId": "a56937f9-b192-4b4d-9451-6cfb6b6be650"
      },
      "execution_count": 122,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "odict_keys(['features.0.weight', 'features.0.bias', 'features.1.weight', 'features.1.bias', 'features.1.running_mean', 'features.1.running_var', 'features.1.num_batches_tracked', 'features.3.weight', 'features.3.bias', 'features.4.weight', 'features.4.bias', 'features.4.running_mean', 'features.4.running_var', 'features.4.num_batches_tracked', 'features.7.weight', 'features.7.bias', 'features.8.weight', 'features.8.bias', 'features.8.running_mean', 'features.8.running_var', 'features.8.num_batches_tracked', 'features.10.weight', 'features.10.bias', 'features.11.weight', 'features.11.bias', 'features.11.running_mean', 'features.11.running_var', 'features.11.num_batches_tracked', 'features.14.weight', 'features.14.bias', 'features.15.weight', 'features.15.bias', 'features.15.running_mean', 'features.15.running_var', 'features.15.num_batches_tracked', 'features.17.weight', 'features.17.bias', 'features.18.weight', 'features.18.bias', 'features.18.running_mean', 'features.18.running_var', 'features.18.num_batches_tracked', 'features.20.weight', 'features.20.bias', 'features.21.weight', 'features.21.bias', 'features.21.running_mean', 'features.21.running_var', 'features.21.num_batches_tracked', 'features.23.weight', 'features.23.bias', 'features.24.weight', 'features.24.bias', 'features.24.running_mean', 'features.24.running_var', 'features.24.num_batches_tracked', 'features.27.weight', 'features.27.bias', 'features.28.weight', 'features.28.bias', 'features.28.running_mean', 'features.28.running_var', 'features.28.num_batches_tracked', 'features.30.weight', 'features.30.bias', 'features.31.weight', 'features.31.bias', 'features.31.running_mean', 'features.31.running_var', 'features.31.num_batches_tracked', 'features.33.weight', 'features.33.bias', 'features.34.weight', 'features.34.bias', 'features.34.running_mean', 'features.34.running_var', 'features.34.num_batches_tracked', 'features.36.weight', 'features.36.bias', 'features.37.weight', 'features.37.bias', 'features.37.running_mean', 'features.37.running_var', 'features.37.num_batches_tracked', 'features.40.weight', 'features.40.bias', 'features.41.weight', 'features.41.bias', 'features.41.running_mean', 'features.41.running_var', 'features.41.num_batches_tracked', 'features.43.weight', 'features.43.bias', 'features.44.weight', 'features.44.bias', 'features.44.running_mean', 'features.44.running_var', 'features.44.num_batches_tracked', 'features.46.weight', 'features.46.bias', 'features.47.weight', 'features.47.bias', 'features.47.running_mean', 'features.47.running_var', 'features.47.num_batches_tracked', 'features.49.weight', 'features.49.bias', 'features.50.weight', 'features.50.bias', 'features.50.running_mean', 'features.50.running_var', 'features.50.num_batches_tracked', 'classifier.0.weight', 'classifier.0.bias', 'classifier.2.weight', 'classifier.2.bias', 'classifier.4.weight', 'classifier.4.bias'])"
            ]
          },
          "metadata": {},
          "execution_count": 122
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "global_model.state_dict()['features.0.weight']"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "uPX_RYRIe56m",
        "outputId": "af10134f-d9b1-4daf-8d8c-b6b65f7d9ca1"
      },
      "execution_count": 123,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "tensor([[[[ 0.1430, -0.0403, -0.0992],\n",
              "          [ 0.0137, -0.0334,  0.1496],\n",
              "          [ 0.0715,  0.0444,  0.0633]],\n",
              "\n",
              "         [[ 0.1898,  0.1119,  0.1620],\n",
              "          [ 0.0805,  0.0311, -0.0513],\n",
              "          [ 0.0409, -0.1489,  0.1449]],\n",
              "\n",
              "         [[-0.0779, -0.0619,  0.0919],\n",
              "          [-0.0861,  0.1827,  0.0919],\n",
              "          [-0.0201, -0.0285,  0.0824]]],\n",
              "\n",
              "\n",
              "        [[[ 0.0686, -0.1898, -0.0114],\n",
              "          [ 0.0943,  0.1052,  0.0698],\n",
              "          [-0.0813, -0.0034,  0.1622]],\n",
              "\n",
              "         [[-0.0069,  0.0103,  0.0844],\n",
              "          [ 0.0920, -0.1111,  0.1512],\n",
              "          [ 0.0348,  0.1457,  0.1795]],\n",
              "\n",
              "         [[ 0.0803,  0.0080,  0.1231],\n",
              "          [ 0.0350, -0.0426, -0.1458],\n",
              "          [ 0.1622,  0.0887,  0.1495]]],\n",
              "\n",
              "\n",
              "        [[[ 0.0567, -0.0378,  0.0183],\n",
              "          [ 0.1905, -0.1672,  0.1884],\n",
              "          [ 0.0363, -0.0400,  0.1214]],\n",
              "\n",
              "         [[-0.1369,  0.1198,  0.0906],\n",
              "          [-0.0950,  0.1273, -0.1112],\n",
              "          [ 0.1137, -0.0226,  0.1269]],\n",
              "\n",
              "         [[-0.1528,  0.0923, -0.1496],\n",
              "          [ 0.1748,  0.0369,  0.0776],\n",
              "          [-0.0208,  0.0405, -0.0100]]],\n",
              "\n",
              "\n",
              "        ...,\n",
              "\n",
              "\n",
              "        [[[ 0.1080, -0.1210, -0.0373],\n",
              "          [ 0.1212,  0.1292,  0.0585],\n",
              "          [-0.0108, -0.1038,  0.1600]],\n",
              "\n",
              "         [[ 0.0012,  0.1138, -0.0087],\n",
              "          [-0.0189, -0.0714, -0.0258],\n",
              "          [ 0.1079,  0.0767,  0.0816]],\n",
              "\n",
              "         [[ 0.1685,  0.0248, -0.1349],\n",
              "          [ 0.1513, -0.0404, -0.0514],\n",
              "          [-0.0894,  0.1265, -0.1471]]],\n",
              "\n",
              "\n",
              "        [[[-0.1637,  0.0863,  0.0274],\n",
              "          [ 0.1334, -0.0226,  0.0826],\n",
              "          [ 0.0341,  0.0365, -0.0149]],\n",
              "\n",
              "         [[-0.0176,  0.1890,  0.0854],\n",
              "          [ 0.0060, -0.1416,  0.1695],\n",
              "          [ 0.1257, -0.1691,  0.1720]],\n",
              "\n",
              "         [[ 0.0460,  0.1211, -0.0800],\n",
              "          [ 0.1919, -0.0076,  0.0217],\n",
              "          [ 0.0044, -0.0767,  0.1127]]],\n",
              "\n",
              "\n",
              "        [[[-0.0177,  0.1914,  0.0379],\n",
              "          [ 0.0718,  0.1652,  0.1764],\n",
              "          [-0.1712, -0.0829, -0.0775]],\n",
              "\n",
              "         [[-0.0472,  0.0362,  0.0853],\n",
              "          [ 0.1656,  0.1050,  0.0850],\n",
              "          [ 0.0795,  0.1889, -0.0637]],\n",
              "\n",
              "         [[-0.0463,  0.0302,  0.1273],\n",
              "          [-0.0023, -0.0464, -0.0590],\n",
              "          [-0.0169, -0.1751, -0.0830]]]])"
            ]
          },
          "metadata": {},
          "execution_count": 123
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def server_aggregate(global_model, client_models):\n",
        "  \"\"\"\n",
        "  This function has aggregation method 'mean'\n",
        "  \"\"\"\n",
        "  ### This will take simple mean of the weights of models ###\n",
        "  global_dict = global_model.state_dict()\n",
        "  for k in global_dict.keys():\n",
        "    global_dict[k] = torch.stack([client_models[i].state_dict()[k].float() for i in range(len(client_models))],0).mean(0)\n",
        "  \n",
        "  # update the server model and clients model\n",
        "  global_model.load_state_dict(global_dict)\n",
        "  for model in client_models:\n",
        "    model.load_state_dict(global_model.state_dict())\n"
      ],
      "metadata": {
        "id": "MSX5_fddSLPR"
      },
      "execution_count": 124,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "The test function is the standard function, which takes the global model along with the test loader as the input and returns the test loss and accuracy."
      ],
      "metadata": {
        "id": "lcZ74HYrgtM-"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def test(global_model,test_loader):\n",
        "  \"\"\"This function test the global model on test data and returns test loss and test accuracy \"\"\"\n",
        "  \n",
        "  global_model.eval()\n",
        "  test_loss = 0\n",
        "  correct = 0\n",
        "  with torch.no_grad():\n",
        "      for data, target in test_loader:\n",
        "          data, target = data.to(device), target.to(device)\n",
        "          output = global_model(data)\n",
        "          test_loss += F.nll_loss(output, target, reduction='sum').item()  # sum up batch loss\n",
        "          pred = output.argmax(dim=1, keepdim=True)  # get the index of the max log-probability\n",
        "          correct += pred.eq(target.view_as(pred)).sum().item()\n",
        "\n",
        "  test_loss /= len(test_loader.dataset)\n",
        "  acc = correct / len(test_loader.dataset)\n",
        "\n",
        "  return test_loss, acc\n"
      ],
      "metadata": {
        "id": "RahDvBgOgqn9"
      },
      "execution_count": 125,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**6. Training the model**\n",
        "\n",
        "One global model, along with the individual client_models is initialized with VGG19 on a GPU. In this tutorial, SGD is used as an optimizer for all the client models."
      ],
      "metadata": {
        "id": "9Gdpd9pUi3nV"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "############################################\n",
        "#### Initializing models and optimizer  ####\n",
        "############################################\n",
        "\n",
        "#### global model ##########\n",
        "global_model =  VGG('VGG19').to(device)\n",
        "\n",
        "############## client models ##############\n",
        "client_models = [ VGG('VGG19').to(device) for _ in range(num_selected)]\n",
        "for model in client_models:\n",
        "    model.load_state_dict(global_model.state_dict()) ### initial synchronizing with global model \n",
        "\n",
        "############### optimizers ################\n",
        "opt = [optim.SGD(model.parameters(), lr=0.1) for model in client_models]"
      ],
      "metadata": {
        "id": "b8EZXgRNi1Eu"
      },
      "execution_count": 126,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Instead of VGG19, one can also use VGG11, VGG13, and VGG16. Other optimizers are also available and one can check the link for more details."
      ],
      "metadata": {
        "id": "f8LrUIYWj7gO"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "np.random.permutation(num_clients)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "C3xF0NoGk9vE",
        "outputId": "40beeb9a-74ba-4780-a4cb-5ab1e02ae783"
      },
      "execution_count": 127,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array([ 4, 15, 16,  1,  8,  0,  9, 18, 12, 19, 11,  2,  6,  7,  5, 17, 10,\n",
              "        3, 14, 13])"
            ]
          },
          "metadata": {},
          "execution_count": 127
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "for i in tqdm(range(3)):\n",
        "  print(i)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "MFpD8MWdkgmY",
        "outputId": "72a58673-668b-46a9-d53d-138227b6d747"
      },
      "execution_count": 128,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 3/3 [00:00<00:00, 18808.54it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "0\n",
            "1\n",
            "2\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "50%26"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "pQ_SvCoD7-Zi",
        "outputId": "84379814-a249-45c1-acb0-79ca1e513304"
      },
      "execution_count": 129,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "24"
            ]
          },
          "metadata": {},
          "execution_count": 129
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "###### List containing info about learning #########\n",
        "losses_train = []\n",
        "losses_test = []\n",
        "acc_train = []\n",
        "acc_test = []\n",
        "# Runnining FL\n",
        "\n",
        "for r in range(num_rounds):\n",
        "    # select random clients\n",
        "    client_idx = np.random.permutation(num_clients)[:num_selected]\n",
        "    # client update\n",
        "    loss = 0\n",
        "    for i in tqdm(range(num_selected)):\n",
        "        loss += client_update(client_models[i], opt[i], train_loader[client_idx[i]], epoch=epochs)\n",
        "    \n",
        "    losses_train.append(loss)\n",
        "    # server aggregate\n",
        "    server_aggregate(global_model, client_models)\n",
        "    \n",
        "    test_loss, acc = test(global_model, test_loader)\n",
        "    losses_test.append(test_loss)\n",
        "    acc_test.append(acc)\n",
        "    if (r % 10 == 0) or (r == (num_rounds-1)):\n",
        "      print('%d-th round' % r)\n",
        "      print('average train loss %0.3g | test loss %0.3g | test acc: %0.3f' % (loss / num_selected, test_loss, acc))\n",
        "    "
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ATW2NQVwjbAb",
        "outputId": "202e5f86-cf5f-49ce-e6f9-4349ec24f60e"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 12/12 [00:01<00:00, 10.67it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "0-th round\n",
            "average train loss 2.33 | test loss 2.3 | test acc: 0.100\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 12/12 [00:01<00:00, 11.58it/s]\n",
            "100%|██████████| 12/12 [00:01<00:00, 11.67it/s]\n",
            "100%|██████████| 12/12 [00:01<00:00, 11.57it/s]\n",
            "100%|██████████| 12/12 [00:01<00:00, 11.60it/s]\n",
            "100%|██████████| 12/12 [00:01<00:00, 11.55it/s]\n",
            "100%|██████████| 12/12 [00:01<00:00, 11.43it/s]\n",
            "100%|██████████| 12/12 [00:01<00:00, 11.51it/s]\n",
            "100%|██████████| 12/12 [00:01<00:00,  9.92it/s]\n",
            "100%|██████████| 12/12 [00:01<00:00, 11.55it/s]\n",
            "100%|██████████| 12/12 [00:01<00:00,  8.92it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "10-th round\n",
            "average train loss 2.13 | test loss 2.14 | test acc: 0.184\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 12/12 [00:01<00:00, 11.65it/s]\n",
            "100%|██████████| 12/12 [00:01<00:00, 10.37it/s]\n",
            "100%|██████████| 12/12 [00:01<00:00, 11.66it/s]\n",
            "100%|██████████| 12/12 [00:01<00:00, 11.64it/s]\n",
            "  0%|          | 0/12 [00:00<?, ?it/s]"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "epochs = 5\n",
        "batch_size = 32\n",
        "\n",
        "train_loader = torch.utils.data.DataLoader(train_data,batch_size=batch_size,shuffle=True)\n",
        "model =  VGG('VGG19')\n",
        "model.to(device)\n",
        "optimizer = optim.SGD(model.parameters(), lr=0.1)\n",
        "\n",
        "for e in range(epochs):\n",
        "  for batch_idx, (data,target) in enumerate(train_loader):\n",
        "    data,target = data.to(device), target.to(device)\n",
        "    optimizer.zero_grad()\n",
        "    output = model(data)\n",
        "    loss = F.nll_loss(output,target)\n",
        "    loss.backward()\n",
        "    optimizer.step()\n",
        "\n",
        "  model.eval()\n",
        "  test_loss = 0\n",
        "  correct = 0\n",
        "  with torch.no_grad():\n",
        "      for data, target in test_loader:\n",
        "          data, target = data.cuda(), target.cuda()\n",
        "          output = model(data)\n",
        "          test_loss += F.nll_loss(output, target, reduction='sum').item()  # sum up batch loss\n",
        "          pred = output.argmax(dim=1, keepdim=True)  # get the index of the max log-probability\n",
        "          correct += pred.eq(target.view_as(pred)).sum().item()\n",
        "  test_loss /= len(test_loader.dataset)\n",
        "  acc = correct / len(test_loader.dataset)\n",
        "  print('%d-th epoch' % e)\n",
        "  print('average train loss %0.3g | test loss %0.3g | test acc: %0.3f' % (loss.item(), test_loss, acc))\n",
        "  model.train()"
      ],
      "metadata": {
        "id": "wAtqb98-rHr4"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}